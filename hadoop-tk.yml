- name: Setting all nodes
  hosts: tknodes
  become: true
  become_method: sudo
  vars:
    local_path: ~/tk
    spark_home: /usr/local/spark
    hadoop_home: /usr/local/hadoop
    java_home: /usr/java/jdk1.8.0_201-amd64
    hadoop_user: wp3hadoop
    master_node:
        wp3node04:
            local_host_name: hadoopnode4
            local_ip: 192.168.0.4
    slave_nodes:
        wp3node01:
            local_host_name: hadoopnode1
            local_ip: 192.168.0.1
        wp3node02:
            local_host_name: hadoopnode2
            local_ip: 192.168.0.2
        wp3node03:
            local_host_name: hadoopnode3
            local_ip: 192.168.0.3
        wp3node05:
            local_host_name: hadoopnode5
            local_ip: 192.168.0.5
        wp3node06:
            local_host_name: hadoopnode6
            local_ip: 192.168.0.6
        wp3node07:
            local_host_name: hadoopnode7
            local_ip: 192.168.0.7
    all_nodes: "{{master_node|combine(slave_nodes)}}"
  tasks:
    - name: "Determine if /mnt/{{inventory_hostname}}vol01 directory exists"
      stat:
        path: "/mnt/{{inventory_hostname}}vol01"
      register: externalVolume
    - name: Setting variables
      set_fact:
        hadoop_state_base_dir: "{{ (externalVolume.stat.exists) | ternary('/mnt/'+inventory_hostname+'vol01/hadoop','/var/lib/hadoop') }}"
    - debug:
        msg: System {{ inventory_hostname }} has hadoop_state_base_dir {{ hadoop_state_base_dir }}
    - name: Stopping services
      command: "{{ item }}"
      become: true
      become_user: "{{ hadoop_user }}"
      ignore_errors: yes
      with_items:
        - "{{spark_home}}/sbin/stop-history-server.sh"
        - "{{hadoop_home}}/sbin/mr-jobhistory-daemon.sh stop historyserver"
        - "{{hadoop_home}}/sbin/stop-yarn.sh"
        - "{{hadoop_home}}/sbin/stop-dfs.sh"
      when: inventory_hostname == master_node.keys()[0]
    #- name: "TEMPORARY deleting {{hadoop_state_base_dir}}"
    #  file:
    #    path: "{{hadoop_state_base_dir}}"
    #    state: absent
    #- name: "TEMPORARY deleting logs"
    #  file:
    #    path: "{{ item }}"
    #    state: absent
    #  with_fileglob:
    #    - "{{hadoop_home}}/logs/*"
    - name: Determine if Java is installed
      stat:
        path: "{{java_home}}/bin/java"
      register: javaInstallation
    - name: Determine if Hadoop is installed
      stat:
        path: "{{hadoop_home}}/LICENSE.txt"
      register: hadoopInstallation
    - name: Add mappings to /etc/hosts
      blockinfile:
        path: /etc/hosts
        block: |
          {{ item.value.local_ip }} {{ item.value.local_host_name }}
        marker: "# {mark} ANSIBLE MANAGED BLOCK {{ item.value.local_host_name }}"
      loop: "{{ lookup('dict', all_nodes) }}"
    - name: "Verify {{hadoop_state_base_dir}} exists"
      file:
        path: "{{hadoop_state_base_dir}}"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
    - name: "Verify {{hadoop_state_base_dir}}/hdfs exists"
      file:
        path: "{{hadoop_state_base_dir}}/hdfs"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
    - name: "Verify {{hadoop_state_base_dir}}/hdfs/datanode exists"
      file:
        path: "{{hadoop_state_base_dir}}/hdfs/datanode"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
    - name: "Verify {{hadoop_state_base_dir}}/yarn exists"
      file:
        path: "{{hadoop_state_base_dir}}/yarn"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
    - name: "Verify {{hadoop_state_base_dir}}/yarn/local exists"
      file:
        path: "{{hadoop_state_base_dir}}/yarn/local"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
    - name: "Verify {{hadoop_state_base_dir}}/yarn/log exists"
      file:
        path: "{{hadoop_state_base_dir}}/yarn/log"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
    - name: Copying Oracle Java rpm to hosts
      copy:
        src: "{{local_path}}/jdk-8u201-linux-x64.rpm"
        dest: /tmp/jdk-8u201-linux-x64.rpm
      when: not javaInstallation.stat.exists
    - name: Installing Oracle Java rpm to hosts
      yum:
        name: /tmp/jdk-8u201-linux-x64.rpm
        state: present
      when: not javaInstallation.stat.exists
    - name: Setting java and javac alternatives
      command: "{{ item }}"
      with_items:
        - update-alternatives --install "/usr/bin/java" "java" "{{java_home}}/bin/java" 0
        - update-alternatives --install "/usr/bin/javac" "javac" "{{java_home}}/bin/javac" 0
        - "update-alternatives --set java {{java_home}}/bin/java"
        - "update-alternatives --set javac {{java_home}}/bin/javac"
      when: not javaInstallation.stat.exists
    - name: Copying and unzipping Hadoop distribution
      unarchive:
        src: "{{local_path}}/hadoop-2.9.2.tar.gz"
        dest: /usr/local
      when: not hadoopInstallation.stat.exists
    - name: Moving Hadoop executables to HADOOP_HOME dir
      command: "mv /usr/local/hadoop-2.9.2 {{hadoop_home}}"
      when: not hadoopInstallation.stat.exists
    - name: Creating or modifying /etc/profile.d/hadoop_setup.sh
      blockinfile:
        path: /etc/profile.d/hadoop_setup.sh
        create: yes
        owner: root
        group: root
        mode: '775'
        block: |
            export PATH=$PATH:{{java_home}}/bin:{{java_home}}/jre/bin
            export J2SDKDIR={{java_home}}
            export J2REDIR={{java_home}}/jre
            export JAVA_HOME={{java_home}}
            export HADOOP_HOME={{hadoop_home}}
            export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
            export HADOOP_MAPRED_HOME=$HADOOP_HOME
            export HADOOP_COMMON_HOME=$HADOOP_HOME
            export HADOOP_HDFS_HOME=$HADOOP_HOME
            export YARN_HOME=$HADOOP_HOME
            export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
            export CLASSPATH=$CLASSPATH:{{hadoop_home}}/lib/*:.
            export HADOOP_OPTS="$HADOOP_OPTS -Djava.security.egd=file:/dev/../dev/urandom -Djava.library.path=$HADOOP_HOME/lib/native"
            export SPARK_HOME={{spark_home}}
            export PATH=$PATH:$SPARK_HOME/bin
            export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH
            export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
            export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
    - name: Setting JAVA_HOME in hadoop-env.sh
      lineinfile: 
          dest: "{{hadoop_home}}/etc/hadoop/hadoop-env.sh"
          regexp: '^export JAVA_HOME='
          line: "export JAVA_HOME={{java_home}}"
          backrefs: yes
    - name: Copying core-site.xml to hosts
      copy:
        src: "{{local_path}}/empty-conf.xml"
        dest: "{{hadoop_home}}/etc/hadoop/core-site.xml"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
    - name: Copying hdfs-site.xml to hosts
      copy:
        src: "{{local_path}}/empty-conf.xml"
        dest: "{{hadoop_home}}/etc/hadoop/hdfs-site.xml"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
    - name: Copying mapred-site.xml to hosts
      copy:
        src: "{{local_path}}/empty-conf.xml"
        dest: "{{hadoop_home}}/etc/hadoop/mapred-site.xml"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
    - name: Copying yarn-site.xml to hosts
      copy:
        src: "{{local_path}}/empty-conf.xml"
        dest: "{{hadoop_home}}/etc/hadoop/yarn-site.xml"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
    - name: Add properties to core-site.xml
      xml:
        path: "{{hadoop_home}}/etc/hadoop/core-site.xml"
        xpath: /configuration
        pretty_print: yes
        set_children:
            - property:
                _:
                    - name: "fs.defaultFS"
                    - value: "hdfs://{{master_node.values()[0].local_host_name}}:8020/"
            - property:
                _:
                    - name: io.file.buffer.size
                    - value: "131072"
    - name: Add properties to hdfs-site.xml
      xml:
        path: "{{hadoop_home}}/etc/hadoop/hdfs-site.xml"
        xpath: /configuration
        pretty_print: yes
        set_children:
            - property:
                _:
                    - name: dfs.namenode.http-bind-host
                    - value: "{{master_node.values()[0].local_host_name}}"
            - property:
                _:
                    - name: dfs.namenode.https-bind-host
                    - value: "{{master_node.values()[0].local_host_name}}"
            - property:
                _:
                    - name: dfs.namenode.rpc-address
                    - value: "{{master_node.values()[0].local_host_name}}:8020"
            - property:
                _:
                    - name: dfs.namenode.servicerpc-address
                    - value: "{{master_node.values()[0].local_host_name}}:9000"
            - property:
                _:
                    - name: dfs.namenode.backup.address
                    - value: "{{master_node.values()[0].local_host_name}}:50100"
            - property:
                _:
                    - name: dfs.namenode.backup.http-address
                    - value: "{{master_node.values()[0].local_host_name}}:50105"
            - property:
                _:
                    - name: dfs.namenode.https-address
                    - value: "{{master_node.values()[0].local_host_name}}:50470"
            - property:
                _:
                    - name: dfs.namenode.secondary.http-address
                    - value: "{{master_node.values()[0].local_host_name}}:50090"
            - property:
                _:
                    - name: dfs.namenode.secondary.https-address
                    - value: "{{master_node.values()[0].local_host_name}}:50091"
            - property:
                _:
                    - name: dfs.namenode.http-address
                    - value: "{{master_node.values()[0].local_host_name}}:50070"
            - property:
                _:
                    - name: dfs.namenode.rpc-bind-host
                    - value: "{{master_node.values()[0].local_host_name}}"
            - property:
                _:
                    - name: dfs.namenode.servicerpc-bind-host
                    - value: "{{master_node.values()[0].local_host_name}}"
            - property:
                _:
                    - name: dfs.namenode.name.dir
                    - value: "file:{{hadoop_state_base_dir}}/hdfs/namenode"
            - property:
                _:
                    - name: dfs.namenode.checkpoint.dir
                    - value: "file:{{hadoop_state_base_dir}}/hdfs/checkpoints"
            - property:
                _:
                    - name: dfs.replication
                    - value: "1"
            - property:
                _:
                    - name: dfs.block.size
                    - value: "134217728"
    - name: Add datanode properties to hdfs-site.xml
      when: inventory_hostname != master_node.keys()[0]
      xml:
        path: "{{hadoop_home}}/etc/hadoop/hdfs-site.xml"
        xpath: /configuration
        pretty_print: yes
        add_children:
            - property:
                _:
                    - name: dfs.datanode.https.address
                    - value: "{{all_nodes[inventory_hostname].local_host_name}}:50475"
            - property:
                _:
                    - name: dfs.datanode.address
                    - value: "{{all_nodes[inventory_hostname].local_host_name}}:50010"
            - property:
                _:
                    - name: dfs.datanode.http.address
                    - value: "{{all_nodes[inventory_hostname].local_host_name}}:50075"
            - property:
                _:
                    - name: dfs.datanode.ipc.address
                    - value: "{{all_nodes[inventory_hostname].local_host_name}}:50020"
            - property:
                _:
                    - name: dfs.datanode.data.dir
                    - value: "file:{{hadoop_state_base_dir}}/hdfs/datanode"
    - name: Add properties to mapred-site.xml
      xml:
        path: "{{hadoop_home}}/etc/hadoop/mapred-site.xml"
        xpath: /configuration
        pretty_print: yes
        set_children:
            - property:
                _:
                    - name: mapred.job.tracker
                    - value: "{{master_node.values()[0].local_host_name}}:9001"
            - property:
                _:
                    - name: mapreduce.framework.name
                    - value: yarn
            - property:
                _:
                    - name: mapreduce.jobhistory.address
                    - value: "{{master_node.values()[0].local_host_name}}:10020"
            - property:
                _:
                    - name: mapreduce.jobhistory.webapp.address
                    - value: "{{master_node.values()[0].local_host_name}}:19888"
            - property:
                _:
                    - name: yarn.app.mapreduce.am.staging-dir
                    - value: /user/app
            - property:
                _:
                    - name: mapred.child.java.opts
                    - value: -Djava.security.egd=file:/dev/../dev/urandom
            - property:
                _:
                    - name: mapreduce.job.user.classpath.first
                    - value: "true"
            - property:
                _:
                    - name: mapreduce.map.memory.mb
                    - value: "2560"
            - property:
                _:
                    - name: mapreduce.reduce.memory.mb
                    - value: "2560"
            - property:
                _:
                    - name: mapreduce.map.java.opts
                    - value: "-Xmx2048m"
            - property:
                _:
                    - name: mapreduce.reduce.java.opts
                    - value: "-Xmx2048m"
    - name: Add properties to yarn-site.xml
      xml:
        path: "{{hadoop_home}}/etc/hadoop/yarn-site.xml"
        xpath: /configuration
        pretty_print: yes
        set_children:
            - property:
                _:
                    - name: yarn.log.server.url
                    - value: "http://{{master_node.values()[0].local_host_name}}:19888/jobhistory/logs"
            - property:
                _:
                    - name: yarn.resourcemanager.hostname
                    - value: "{{master_node.values()[0].local_host_name}}"
            - property:
                _:
                    - name: yarn.resourcemanager.bind-host
                    - value: "{{master_node.values()[0].local_host_name}}"
            - property:
                _:
                    - name: yarn.scheduler.minimum-allocation-mb
                    - value: "1024"
            - property:
                _:
                    - name: yarn.scheduler.maximum-allocation-mb
                    - value: "3584"
            - property:
                _:
                    - name: yarn.log-aggregation-enable
                    - value: "true"
    - name: Add nodemanager properties to yarn-site.xml
      when: inventory_hostname != master_node.keys()[0]
      xml:
        path: "{{hadoop_home}}/etc/hadoop/yarn-site.xml"
        xpath: /configuration
        pretty_print: yes
        add_children:
            - property:
                _:
                    - name: yarn.nodemanager.log-aggregation.compression-type
                    - value: gz
            - property:
                _:
                    - name: yarn.nodemanager.hostname
                    - value: "{{all_nodes[inventory_hostname].local_host_name}}"
            - property:
                _:
                    - name: yarn.nodemanager.bind-host
                    - value: "{{all_nodes[inventory_hostname].local_host_name}}"
            - property:
                _:
                    - name: yarn.nodemanager.aux-services
                    - value: mapreduce_shuffle,spark_shuffle
            - property:
                _:
                    - name: yarn.nodemanager.aux-services.mapreduce_shuffle.class
                    - value: org.apache.hadoop.mapred.ShuffleHandler
            - property:
                _:
                    - name: yarn.nodemanager.aux-services.spark_shuffle.class
                    - value: org.apache.spark.network.yarn.YarnShuffleService
            - property:
                _:
                    - name: yarn.nodemanager.aux-services.spark_shuffle.classpath
                    - value: "{{hadoop_home}}/lib/spark-2.4.3-yarn-shuffle.jar"
            - property:
                _:
                    - name: yarn.nodemanager.local-dirs
                    - value: "file:{{hadoop_state_base_dir}}/yarn/local"
            - property:
                _:
                    - name: yarn.nodemanager.log-dirs
                    - value: "file:{{hadoop_state_base_dir}}/yarn/log"
            - property:
                _:
                    - name: yarn.nodemanager.remote-app-log-dir
                    - value: "hdfs://{{master_node.values()[0].local_host_name}}:8020/var/log/hadoop-yarn/apps"
            - property:
                _:
                    - name: yarn.nodemanager.vmem-check-enabled
                    - value: "false"
            - property:
                _:
                    - name: yarn.nodemanager.vmem-pmem-ratio
                    - value: "4"
            - property:
                _:
                    - name: yarn.nodemanager.resource.cpu-vcores
                    - value: "2"
            - property:
                _:
                    - name: yarn.nodemanager.resource.memory-mb
                    - value: "3584"
    - name: Fixing permissions on HADOOP_HOME dir
      file:
        path: "{{hadoop_home}}"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        recurse: yes
        state: directory
    - name: Determine if Spark is installed
      stat:
        path: "{{spark_home}}/LICENSE"
      register: sparkInstallation
      when: inventory_hostname == master_node.keys()[0]
    - name: "Verify {{hadoop_state_base_dir}}/hdfs/namenode exists"
      file:
        path: "{{hadoop_state_base_dir}}/hdfs/namenode"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
      when: inventory_hostname == master_node.keys()[0]
    - name: "Verify {{hadoop_state_base_dir}}/hdfs/checkpoints exists"
      file:
        path: "{{hadoop_state_base_dir}}/hdfs/checkpoints"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: "u+rw,g+rw,o-rwx"
        state: directory
      when: inventory_hostname == master_node.keys()[0]
    - name: "Delete {{hadoop_home}}/etc/hadoop/masters if exists"
      file:
        path: "{{hadoop_home}}/etc/hadoop/masters"
        state: absent
      when: inventory_hostname == master_node.keys()[0]
    - name: "Delete {{hadoop_home}}/etc/hadoop/slaves if exists"
      file:
        path: "{{hadoop_home}}/etc/hadoop/slaves"
        state: absent
      when: inventory_hostname == master_node.keys()[0]
    - name: "Add hosts to {{hadoop_home}}/etc/hadoop/masters"
      blockinfile:
        path: "{{hadoop_home}}/etc/hadoop/masters"
        create: yes
        block: |
          {{master_node.values()[0].local_host_name}}
        marker: "# {mark} ANSIBLE MANAGED BLOCK {{master_node.values()[0].local_host_name}}"
      when: inventory_hostname == master_node.keys()[0]
    - name: "Add hosts to {{hadoop_home}}/etc/hadoop/slaves"
      blockinfile:
        path: "{{hadoop_home}}/etc/hadoop/slaves"
        create: yes
        block: |
          {{ item.value.local_host_name }}
        marker: "# {mark} ANSIBLE MANAGED BLOCK {{ item.value.local_host_name }}"
      loop: "{{slave_nodes | dict2items }}"
      when: inventory_hostname == master_node.keys()[0]
    - name: Copying and unzipping Spark distribution
      unarchive:
        src: "{{local_path}}/spark-2.4.3-bin-hadoop2.7.tgz"
        dest: /usr/local
      when: inventory_hostname == master_node.keys()[0] and not sparkInstallation.stat.exists
    - name: Moving Spark executables to SPARK_HOME dir
      command: "mv /usr/local/spark-2.4.3-bin-hadoop2.7 {{spark_home}}"
      when: inventory_hostname == master_node.keys()[0] and not sparkInstallation.stat.exists
    - name: Modifying spark-env.sh
      blockinfile:
        path: "{{spark_home}}/conf/spark-env.sh"
        create: yes
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        block: |
          export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.eventLog.enabled=true -Dspark.eventLog.dir=hdfs://{{master_node.values()[0].local_host_name}}:8020/var/log/spark -Dspark.history.fs.logDirectory=hdfs://{{master_node.values()[0].local_host_name}}:8020/var/log/spark"
          export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
          export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
      when: inventory_hostname == master_node.keys()[0]
    - name: Modifying spark-defaults.conf
      blockinfile:
        path: "{{spark_home}}/conf/spark-defaults.conf"
        create: yes
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        block: |
          spark.yarn.archive                      hdfs:///spark-jars/jars.tar.gz
          spark.master                            yarn
          spark.serializer                        org.apache.spark.serializer.KryoSerializer
          spark.eventLog.enabled                  true
          spark.eventLog.dir                      hdfs://{{master_node.values()[0].local_host_name}}:8020/var/log/spark
          spark.yarn.historyServer.address        {{master_node.values()[0].local_host_name}}:18080
          spark.shuffle.service.enabled           true
          spark.executor.cores                    2
          spark.executor.instances                2
      when: inventory_hostname == master_node.keys()[0]
    - name: Fixing permissions on SPARK_HOME dir
      file:
        path: "{{spark_home}}"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        recurse: yes
        state: directory
      when: inventory_hostname == master_node.keys()[0]
    - name: "Determine if {{hadoop_home}}/lib/spark-2.4.3-yarn-shuffle.jar exists"
      become: true
      become_user: "{{ hadoop_user }}"
      stat:
        path: "{{hadoop_home}}/lib/spark-2.4.3-yarn-shuffle.jar"
      register: sparkYarnShuffleFile
    - name: Copying spark-2.4.3-yarn-shuffle.jar to hadoop libs
      become: true
      become_user: "{{ hadoop_user }}"
      copy:
        src: "{{local_path}}/spark-2.4.3-yarn-shuffle.jar"
        dest: "{{hadoop_home}}/lib/spark-2.4.3-yarn-shuffle.jar"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
      when: not sparkYarnShuffleFile.stat.exists
    - name: Determine if HDFS is formatted
      become: true
      become_user: "{{ hadoop_user }}"
      stat:
        path: "{{hadoop_state_base_dir}}/hdfs/namenode/current"
      register: hdfsFormatted
      when: inventory_hostname == master_node.keys()[0]
    - name: Formatting Namenode
      become: true
      become_user: "{{ hadoop_user }}"
      command: "{{hadoop_home}}/bin/hdfs namenode -format"
      when: inventory_hostname == master_node.keys()[0] and not hdfsFormatted.stat.exists
    - name: Starting services
      become: true
      become_user: "{{ hadoop_user }}"
      command: "{{ item }}"
      with_items:
        - "{{hadoop_home}}/sbin/start-dfs.sh"
        - "{{hadoop_home}}/sbin/start-yarn.sh"
      when: inventory_hostname == master_node.keys()[0]
    - name: Creating HDFS directories
      become: true
      become_user: "{{ hadoop_user }}"
      command: "{{ item }}"
      with_items:
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir /tmp"
        - "{{hadoop_home}}/bin/hdfs dfs -chmod -R 1777 /tmp"
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir /user"
        - "{{hadoop_home}}/bin/hdfs dfs -chmod -R 1777 /user"
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir /user/app"
        - "{{hadoop_home}}/bin/hdfs dfs -chmod -R 1777 /user/app"
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir -p /var/log/hadoop-yarn"
        - "{{hadoop_home}}/bin/hdfs dfs -chmod -R 1777 /var/log/hadoop-yarn"
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir -p /var/log/hadoop-yarn/apps"
        - "{{hadoop_home}}/bin/hdfs dfs -chmod -R 1777 /var/log/hadoop-yarn/apps"
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir /user/{{ hadoop_user }}"
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir /var/log/spark"
        - "{{hadoop_home}}/bin/hdfs dfs -chmod -R 1777 /var/log/spark"
        - "{{hadoop_home}}/bin/hdfs dfs -mkdir /spark-jars"
        - "tar -cvzf /tmp/jars.tar.gz -C {{spark_home}}/jars ."
        - "{{hadoop_home}}/bin/hdfs dfs -put /tmp/jars.tar.gz /spark-jars"
        - "rm /tmp/jars.tar.gz"
      when: inventory_hostname == master_node.keys()[0] and not hdfsFormatted.stat.exists
    - name: Starting history services
      become: true
      become_user: "{{ hadoop_user }}"
      command: "{{ item }}"
      with_items:
        - "{{hadoop_home}}/sbin/mr-jobhistory-daemon.sh start historyserver"
        - "{{spark_home}}/sbin/start-history-server.sh"
      when: inventory_hostname == master_node.keys()[0]
